<h1 align = "center">
  Previs√µes utilizando Grafos
</h1>

Este reposit√≥rio cont√©m a implementa√ß√£o de uma forma primitiva e amadora de realizar previs√µes sobre qual time sairia vencedor em uma partida de futebol, envolvendo todos os times presentes na s√©rie A.

<h2 align = "center">
    ‚ö†Ô∏è
    <strong>
        <em> Problem√°tica </em>
    </strong> 
</h2>

Sabe-se que no futebol, uma das grandes ind√∫strias que crescem √© voltada para o mercado de apostas, sendo uma fonte consideravelmente forte de renda extra. Entretanto, enquanto humanos, √© comum desconsiderar alguns fatores essenciais ao realizar uma aposta por puro esquecimento de estat√≠stica X ou Y. Pensando nisso, o programa foi criado como uma forma de juntar todas as estat√≠sticas importantes para o processo de decis√£o para um resultado, apresentando uma previs√£o decidida por meio da an√°lise de todas as informa√ß√µes coletadas por meio de sites voltados ao mundo futibol√≠stico.

<h2 align = "center">
    ‚öôÔ∏è
    <strong>
        <em>Tecnologias utilizadas</em>
    </strong>
</h2>

<table align="center">
  <tr>
    <td><strong>Tecnologia</strong></td>
    <td><strong>Vers√£o</strong></td>
  </tr>
  <tr>
    <td>Python</td>
    <td>3.10.6</td>
  </tr>
  <tr>
    <td>pandas</td>
    <td>1.5.1</td>
  </tr>
  <tr>
    <td>numpy</td>
    <td>1.23.4</td>
  </tr>
  <tr>
    <td>requests</td>
    <td>2.28.1</td>
  </tr>
  <tr>
    <td>lxml</td>
    <td>4.9.1</td>
  </tr>
  <tr>
    <td>openpyxl</td>
    <td>3.0.10</td>
  </tr>
  <tr>
    <td>networkx</td>
    <td>2.8.8</td>
  </tr>
  <tr>
    <td>matplotlib</td>
    <td>3.6.2</td>
  </tr>
</table>


<h2 align = "center">
    üî¨
    <strong>
        <em>Dados levados em considera√ß√£o</em>
    </strong>
</h2>

Essa √© a vers√£o inicial de um projeto que poss√≠velmente pode ter sequ√™ncia, sendo assim, o programa conta com apenas um tipo de dado levado em considera√ß√£o:
- Coloca√ß√µes nos 5 anos anteriores da competi√ß√£o.

√â plaus√≠vel ainda salientar que houve o desejo de adicionar o m√©rito do confronto direto entre ambos os times que estariam em duelo, entretanto, a fonte encontrada para a desejada informa√ß√£o n√£o aceitava a quantidade de requisi√ß√µes necess√°rias pelo programa. Ao entrar em contato com o site em quest√£o, solicitando uma libera√ß√£o ou se havia a exist√™ncia de uma API, n√£o houve retorno.

<h2 align="center"> 
   üí°<strong>
        <em>Algoritmo </em>
    </strong>
</h2>

<d1>
    <dt> 1. Realizar a coleta de informa√ß√µes desejadas por meio de Web Scraping; </dt>
    <dt> 2. Salvar as informa√ß√µes no programa para uso posterior; </dt>
    <dt> 3. Aplicar a aritm√©tica considerada eficiente aos dados coletados; </dt>
    <dt> 4. Utilizar grafos a fim de organizar as informa√ß√µes da forma desejada; </dt>
    <dt> 5. Solicitar ao usu√°rio que informe qual ser√° o confronto; </dt>
    <dt> 6. Informar ao usu√°rio qual o time que <strong>PROVAVELMENTE</strong> vencer√° o confronto, utilizando os dados j√° armazenados no programa. </dt>
</d1>

<h2 align="center"> 
   üñ•Ô∏è<strong>
        <em>Implementa√ß√£o</em>
    </strong>
</h2>

Inicialmente, seguindo o que foi pontuado no detalhamento do algoritmo, o primeiro passo foi realizar a coleta de informa√ß√µes. Para esse est√°gio, foram utilizadas as bibliotecas **pandas**, **requests**, **openpyxl** e **lxml**, sendo cada uma delas extremamente importante para cada fase dessa coleta:
- **Pandas** ser√° utilizado a fim de trabalhar com todo o conte√∫do HTML em dataframes;
- **Requests** foi utilizado como uma forma segura de realizar as requisi√ß√µes para o Web Scraping a fim de minimizar as chances de erros;
- **Openpyxl** e **lxml** serviu o prop√≥sito de salvar todos os dados coletados, ap√≥s serem tratados, em arquivos **.xlsx**.

Todo esse processo √© feito dentro da fun√ß√£o **_webScraping_Position(x)_** e, inicialmente, √© estruturado um dicion√°rio com o nome **ids_times()**, respons√°vel por associar o time que se deseja realizar a pesquisa com sua identifica√ß√£o √∫nica no site, que ser√° chamado na vari√°vel **url_link**, a qual receber√° o endere√ßo padr√£o do site, com o final sendo a parte que difere cada time e, para realiz√°-la, √© chamado a posi√ß√£o **x** do dicion√°rio (nome do time, par√¢metro da fun√ß√£o em pauta), que concatenar√° o ID ao restante do link.

Na sequ√™ncia √© feita a requisi√ß√£o no site que se deseja obter os dados e assim, todo o conte√∫do HTML √© transferido para uma vari√°vel auxiliar, sendo um array de dataframes, divididos pelas tabelas. Em seguida, a tabela correta √© transferida para uma vari√°vel que ser√° utilizada para realizar o tratamento dos dados. O tratamento foi pensado visando obter somente as informa√ß√µes necess√°rias, al√©m da identifica√ß√£o da temporada e da pontua√ß√£o, apenas por quest√£o de controle e organiza√ß√£o, ao salvar os dados no arquivo .xlsx. Sendo assim, foram selecionados tr√™s colunas, que foram posteriormente renomeadas para melhor visualiza√ß√£o, sendo elas a **Temporada**, **Pontos** e **Coloca√ß√£o**, sendo a coloca√ß√£o a que de fato seria utilizada. Por fim, os dados s√£o salvos no arquivo, assim como anteriormente citado, e a fun√ß√£o tem como retorno a chamada para uma outra fun√ß√£o, **_arithmeticsPositionAverage(var)_**, a qual ser√° respons√°vel por realizar a aritm√©tica desejada utilizando os dados obtidos. A fun√ß√£o de retorno ser√° explicada em seguida, logo ap√≥s a apresenta√ß√£o do c√≥digo que acabara de ser mencionado.

```python
def webScraping_position(x):
	ids_times = {'palmeiras': '1023', 'internacional': '6600', 'fluminense': '2462', 'corinthians': '199', 'flamengo': '614', 'atletico_pr': '679', 'atletico_mg': '330', 
	'america_MG': '2863', 'sao_paulo': '585', 'botafogo': '537','fortaleza': '10870', 'santos': '221', 'goias': '3197', 'bragantino': '8793', 'coritiba': '776', 
	'cuiaba': '28022', 'ceara': '2029', 'atletico_go': '15172', 'avai': '2035', 'juventude': '10492'}

	header = {
		"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36",
		"X-Requested-With": "XMLHttpRequest"
	}

	url_link = "https://www.transfermarkt.com.br/standart-team/platzierungen/verein/" + ids_times[x]

	req = requests.get(url_link, headers = header)
	df = pd.read_html(req.text)
	stats = df[1].copy()
	stats = stats[['Temporada', 'Pontos', 'Liga.2']]
	stats = stats.rename(columns={'Temporada': 'Temporada', 'Pontos': 'Coloca√ß√£o', 'Liga.2': 'Divis√£o'})
	stats.to_excel("position/" + x + ".xlsx")

	return (arithmeticsPositionAverage(stats))
```

Tendo os dados sido coletados, parte ao ponto de realizar as opera√ß√µes com as informa√ß√µes de forma que elas sejam condizentes com a realidade atual, haja visto que os dados voltam s√£o coletados sobre os √∫ltimos 5 anos. A forma encontrada para manter os dados mais realistas poss√≠veis foi por meio de uma m√©dia ponderada, juntamente a algumas outras exce√ß√µes que foram tratadas como forma de manter a integridade do programa:

Basicamente, foram dados pesos regressivos √† medida que os anos passaram, quanto mais atual, maior o peso da coloca√ß√£o, baseado em quantos dados se tem de um determinado time. Ap√≥s as coletas de dados, foi realizado um teste envolvendo todos os times que seriam analisados, a fim de visualizar qual o dado mais antigo que se tem para cada um. Mediante a tal an√°lise, foi poss√≠vel ter a certeza de que o time com mais dados tinha 17 coloca√ß√µes passadas. Ademais, foi poss√≠vel perceber que determinados times n√£o possuiam essa mesma quantidade devido ao fator de n√£o estarem entre as principais ligas do Campeonato Brasileiro, sendo elas S√©rie A e S√©rie B, jogando ainda as classificat√≥rias, as quais n√£o foram abordadas pelo site. Devido √† essa situa√ß√£o, para os times que n√£o possuiam essa mesma quantidade de dados, foi atribuido a coloca√ß√£o 20¬∫, assumindo que o time ficaria na √∫ltima coloca√ß√£o, devido √† grande discrep√¢ncia (S√©rie A vs S√©rie C+). Ainda √© plaus√≠vel salientar que, para os times com coloca√ß√µes na segunda divis√£o, foi atribu√≠da a coloca√ß√£o 16¬∫, desconsiderando a coloca√ß√£o obtida na s√©rie B, como uma forma de, novamente, parear para como se estivessem jogando na primeira divis√£o do futebol brasileiro. Por fim, aritm√©tica seguiu a seguinte regra, se estendendo at√© o ano de 2005:

<table align="center">
  <tr>
    <td><strong>Temporada</strong></td>
    <td><strong>Peso</strong></td>
  </tr>
  <tr>
    <td>2021</td>
    <td>1</td>
  </tr>
  <tr>
    <td>2020</td>
    <td>16/17</td>
  </tr>
  <tr>
    <td>2019</td>
    <td>15/17</td>
  </tr>
  <tr>
    <td>2018</td>
    <td>14/17</td>
  </tr>
  <tr>
    <td>2017</td>
    <td>13/17</td>
  </tr>
</table>

Agora, partindo para como foi para, de fato, implementar a ideia, inicialmente foi utilizada uma vari√°vel para salvar qual era o √∫ltimo index v√°lido, haja visto que ele representaria a quantidade de anos em que os valores foram coletados. Portanto, a vari√°vel **boundary** recebeu o valor de **_stats.last_valid_index() + 1_**, sendo stats o dataframe obtido a partir da fun√ß√£o mencionada anteriormente. O motivo do +1 ao final √© devido ao index come√ßar de 0, e deseja-se iniciar a contagem assim como se realiza fora do √¢mbito da programa√ß√£o (come√ßando de 1), a fim de obter mais controle. Na sequ√™ncia, essa vari√°vel passa por uma condicional e, caso a mesma seja diferente de 17 (n√£o tenha dados de todos os anos), ter√° um tratamento diferente em situa√ß√£o posterior.

Seguindo, chega ao ponto que √© estruturado o denominador, sendo ele o somat√≥rio de todos os pesos utilizados para o numerador. Para tal, √© utilizado um looping **for** simples, num range que vai de 1 a 18, fazendo com que o denominador seja igual a ele mesmo somado a $\frac{j}{17}$, que vai sendo iterado at√© chegar ao peso final 1. 

Tendo o valor do denominador, resta somente obter o valor do numerador e, a fim de seguir o que foi supracitado sobre as coloca√ß√µes e divis√µes, foi utilizada a vari√°vel **lvi** que tem valor inicial igual a 17, que ser√° decrementada a media que as temporadas forem passando, a fim de seguir o que foi visualizado na tabela acima, al√©m de tr√™s condicionais que foram utilizadas dentro de um looping **for**, num range que vai de 0 a 17 para a vari√°vel "i", buscando aplicar a m√©dia ponderada e obter, finalmente a "classifica√ß√£o geral" do time:

- Caso "i" seja menor que **boundary** e a "divis√£o" seja diferente de "Segunda Divis√£o", significa que h√° dados sobre o time na primeira divis√£o no determinado ano e assim
  - Numerador ser√° igual a ele mesmo somado a $colocacao * \frac{lvi}{17}$;
- Caso "i" seja menor que **boundary** e a "divis√£o" seja igual a "Segunda Divis√£o", significa que h√° dados sobre o time na segunda divis√£o no determinado ano e assim
  - Numerador ser√° igual a ele mesmo somado a $\frac{16 * lvi}{17}$;
- Caso "i" seja maior ou igual a **boundary**, significa que n√£o h√° dados sobre o time seja na primeira ou segunda divis√£o no determinado ano, e assim
  - Numerador ser√° igual a ele mesmo somado a $\frac{20 * lvi}{17}$;


Ainda h√° de salientar que, ao final de cada itera√ß√£o, **lvi** √© decrementado em 1. Por fim, a fun√ß√£o retorna o valor de $\frac{numerador}{denumerador}$ para onde a fun√ß√£o foi chamada. Veja abaixo como a fun√ß√£o foi implementada:

```python
def arithmeticsPositionAverage(stats):
	boundary = stats.last_valid_index() + 1
	lvi = 17

	numerator = 0
	j = 1
	denominator = 0

	for j in range(18):
		denominator = denominator + (j/17)

	for i in range(17):
		if i < (boundary) and stats['Divis√£o'].iloc[i] != "Segunda Divis√£o":
			numerator = numerator + (stats['Coloca√ß√£o'].iloc[i] * (lvi / 17))
		elif i < (boundary) and stats['Divis√£o'].iloc[i] == "Segunda Divis√£o":
			numerator = numerator + (16 * (lvi / 17))
		elif i >= boundary:
			numerator = numerator + (20 * (lvi / 17))

		lvi = lvi - 1

	return (numerator/denominator)
```

Agora chega ao ponto onde tudo √© comutado a fim de que o programa de fato aconte√ßa. Na "main", existem dois dicion√°rios, sendo um deles voltado para enviar a fun√ß√µes os exatos nomes utilizados para obter o ID correto dos times ap√≥s serem digitados por meio do Menu, sendo ele o **team_codes**, junto ao **team_output_names**, utilizado com o prop√≥sito de, a partir da sele√ß√£o no Menu, ser capaz de usar o nome do time como output de uma forma mais agrad√°vel visualmente, al√©m de condizente com o portugu√™s escrito. Aliado a isso, h√° uma lista com o nome de todos os times escritos assim como no primeiro dicion√°rio, a fim de passar tamb√©m para a fun√ß√£o com a correta gram√°tica para encontrar a p√°gina correta do time durante o Web Scraping, com o nome de **graph_names**.

Para o in√≠cio da implementa√ß√£o, ainda √© desginado um novo dicion√°rio, entretanto ainda vazio, com o nomve de **average_positions**. Com tudo isso sendo dito, √© iniciado um looping **for**, que itera por todos os elementos da lista supracitada, chamando a fun√ß√£o para fazer o Web Scraping para cada "i", sendo ele o nome dos times, utilizando o pr√≥prio nome como a chave para o dicion√°iro acima, sendo o valor da chave, a m√©dia encontrada pelo programa.

A esse ponto, v√™m a necessidade de explicar como os **Grafos** ser√£o usados para essa aplica√ß√£o:
  - Cada time ser√° um v√©rtice, e eles ser√£o ligados de forma que, caso o time A tenha uma m√©dia **MENOR** do que a m√©dia do time B, uma aresta "sair√°" de A e "entrar√°" em B, indicando que A tem maior probabilidade de ganhar a partida do que B.
  - Todos os times ser√£o comparados com todos,, mesmo que nem todos eles sejam usados, de forma que, uma vez que o programa esteja pronto para uso, todas as a√ß√µes tomadas dentro do mesmo sejam instant√¢neas.

√â criado um grafo direcionado, utilizando a biblioteca **_networkx_** e nele s√£o adicionados todos os n√≥s desejados, sendo eles os times utilizados e presentes na lista **graph_names** (inclusive esse √© o motivo da mesma ter esse nome). Em sequ√™ncia, √© utilizado o m√©todo da bolha, a fim de comparar todos os valores entre si. Nele, √© utilizado as informa√ß√µes do **average_positions**, e visualizado se a m√©dia do time em **average_positions**, com a chave **graph_names** em **i**, √© maior que o mesmo direcionamento, por√©m trocando **i** por **j**:
  - Caso seja, ser√° adicionado uma aresta, saindo do v√©rtice **graph_names[i]** para **graph_names[j]**;
  - Caso contr√°rio, sair√° de **graph_names[j]** para **graph_names[i]**.

Com as informa√ß√µes todas salvas, um menu ser√° inserido na tela do usu√°rio, dando op√ß√µes de times para escolher e realizar as compara√ß√µes, al√©m de um adendo para caso dseje sair do programa, sendo necess√°rio apenas digitar o n√∫mero para time igual, tanto no primeiro, quanto no segundo.

Ap√≥s esse processo, caso seja de desejo do usu√°rio visualizar o Grafo, o programa perguntar√°, bastando digitar **1** quando solicitado, se n√£o, digitando **2**, ser√° finalizado sem o display.

<h2 align="center">
    üñ®Ô∏è <strong>
        <em>Exemplo de Execu√ß√£o</em>
    </strong>
</h2>

Ap√≥s executar corretamente o programa, o que se deve esperar √© um menu como esse, contando com dois inputs por conta do usu√°rio e, logo em seguida, o resultado para o confronto em quest√£o:

<p align="center">
    <img src="imgs/execu√ß√£oexemplo.jpeg" width="800px"/> 
</p>

Ao finalizar a execu√ß√£o do looping, o menu abaixo ser√° disponibilizado ao usu√°rio:

<p align="center">
    <img src="imgs/menufinal.png" width="400px"/> 
</p>

E caso a op√ß√£o selecionada seja a primeira, um grafo desse padr√£o ser√° gerado para posterior visualiza√ß√£o do usu√°rio:

<p align="center">
    <img src="imgs/graph.jpeg" width="800px"/> 
</p>

<h2 align="center">
    üîß
    <strong>
        <em> 
            Execu√ß√£o
        </em>    
    </strong>
</h2>

Inicialmente, verifique se todas as bibliotecas est√£o coerentes com as apresentadas no t√≥pico Tecnologias Utilizadas, abra o console na pasta do reposit√≥rio e siga com o comando:
<p align="center"> <strong> python3 main.py</strong></p>